# 二进制方式部署Kubernetes集群
- 官网`https://kubernetes.io/docs/imported/release`选择对应版本进入下载页面有几个版本：
    - kubernetes.tar.gz 包含了Kubernetes的服务程序文件、文档和示例；
    - kubernetes-src.tar.gz 包含了全部源代码；
    - Server/node/Client  Binaries 各自的服务程序文件；

## 系统及环境说明
搭建环境节点说明：
| 主机名 | 节点IP | 节点角色 | 组件 | 版本 |
| :-- | :-- | :-- | :-- | :-- |
| k8s-master-210 | 192.168.153.210 | master | kube-apiserver<br/>kube-controller-manager<br/>kube-scheculer<br/>coreDNS<br/>dashboard 1.8.0<br/>heapster 1.2.0 | kubernetes v1.9.0 |
| k8s-node-211 | 192.168.153.211 | node1<br/>etcd1 | kubelet<br/>kube-proxy<br/>docker | docker-ce 17.03<br/>etcd-v3.2.11 |
| k8s-node-212 | 192.168.153.212 | node2<br/>etcd2 | kubelet<br/>kube-proxy<br/>docker |docker-ce 17.03<br/>etcd-v3.2.11 |
| k8s-node-213 | 192.168.153.213 | node3<br/>etcd3 | kubelet<br/>kube-proxy<br/>docker |docker-ce 17.03<br/>etcd-v3.2.11 |   |   

> 由于是测试环境etcd集群也放在master和node上、线上环境可以单独开三台机器。搭建步骤都一
样

集群网络结构：
| 网络名称 | 网络范围 |
| :------ | :------ |
| pod-ip | 10.254.128.0/17 |
| cluster-ip | 10.254.0.0/17 |
| node-ip | 192.168.153.0/24 |

master端口开放
| 端口 | 说明 |
| :-- | :--- |
| 6443 | master apiserver通信端口  |

node端口开放
| 端口 | 说明 |
| :-- | :--- |
| 2379 | etcd client连接端口  |
| 2380 | etcd 集群节点通信端口  |

## 初始化环境
### 防火墙 selinux
关闭firewalld、selinux 安装iptables并开启ssh端口。

### 修改主机名
所有机器修改主机名、添加hosts
```
$ hostnamectl --static set-hostname <hostname>
 192.168.153.210 -> k8s-master-210   master-192.168.1.1
 192.168.153.211 -> k8s-node-211     node-192.168.1.2
 192.168.153.212 -> k8s-node-212
 192.168.153.213 -> k8s-node-213

$ vim /etc/hosts
 192.168.153.210 k8s-master-210
 192.168.153.211 k8s-node-211
 192.168.153.212 k8s-node-212
 192.168.153.213 k8s-node-213
```

###  配置ssh互信
为了方便后续分发证书、提前做好master与其他所有节点的ssh互信
```
$ ssh-keygen -t rsa
$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.153.211
$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.153.212
$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.153.213
```

### 创建CA证书
这里使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 证书和秘钥文件
#### 安装cfssl
```
$ mkdir -p /opt/local/cfssl
$ cd /opt/local/cfssl
$ wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
$ wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
$ wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
$ mv cfssl_linux-amd64 cfssl
$ mv cfssljson_linux-amd64 cfssljson
$ mv cfssl-certinfo_linux-amd64 cfssl-certinfo
$ chmod +x ./*
```

#### 创建CA证书配置
```
$ mkdir /opt/ssl
$ cd /opt/ssl
$ cat > /opt/ssl/config.json << "EOF"
{
  "signing": {
    "default": {
      "expiry": "87600h"
    },
    "profiles": {
      "kubernetes": {
        "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ],
        "expiry": "87600h"
      }
    }
  }
}
EOF

$ cat > /opt/ssl/csr.json << "EOF"
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "ShenZhen",
      "L": "ShenZhen",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
EOF
```

#### 生成CA证书和秘钥
```
$ cd /opt/ssl/
$ /opt/local/cfssl/cfssl gencert -initca csr.json | /opt/local/cfssl/cfssljson -bare ca
$ ls
ca.csr  ca-key.pem  ca.pem  config.json  csr.json

# csr 证书签名请求，包含证书持有人的信息
# pem 证书或秘钥的Base64文本存储格式 ca-key 私钥 ca 公钥
```

#### 分发证书
```
# 所有节点都创建证书目录
$ mkdir -p /etc/kubernetes/ssl

# 拷贝所有文件到目录下
$ cp *.pem /etc/kubernetes/ssl
$ cp ca.csr /etc/kubernetes/ssl

# 这里要将文件拷贝到所有的机器上
$ for node in {192.168.153.211,192.168.153.212,192.168.153.213};do \
    rsync -avzp /opt/ssl/*.pem $node:/etc/kubernetes/ssl/; \
    rsync -avzp /opt/ssl/*.csr $node:/etc/kubernetes/ssl/; \
done
```

## 安装及配置docker
### 安装docker
所有服务器预先安装 docker-ce ，官方1.9 中提示， 目前 k8s 支持最高 Docker versions 1.11.2, 1.12.6, 1.13.1, and 17.03.1
```
# 安装 yum-config-manager
$ mkdir /opt/package
$ cd /opt/package
$ yum -y install yum-utils

# 导入yum源
$ yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo

# 更新yum-repo
$ yum makecache

# 查看yum版本
$ yum list docker-ce.x86_64  --showduplicates |sort -r

# 安装指定版本 docker-ce 17.03 被 docker-ce-selinux 依赖, 不能直接yum安装 docker-ce-selinux
$ wget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch.rpm
$ rpm -ivh docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch.rpm
$ yum -y install docker-ce-17.03.1.ce

# 查看安装
$ docker version
```

### 修改docker配置并启动
```
# 添加docker启动配置
$ cat > /etc/systemd/system/docker.service << "EOF"
[Unit]
Description=Docker Application Container Engine
Documentation=http://docs.docker.com
After=network.target docker-storage-setup.service
Wants=docker-storage-setup.service

[Service]
Type=notify
Environment=GOTRACEBACK=crash
ExecReload=/bin/kill -s HUP $MAINPID
Delegate=yes
KillMode=process
ExecStart=/usr/bin/dockerd \
          $DOCKER_OPTS \
          $DOCKER_STORAGE_OPTIONS \
          $DOCKER_NETWORK_OPTIONS \
          $DOCKER_DNS_OPTIONS \
          $INSECURE_REGISTRY
LimitNOFILE=1048576
LimitNPROC=1048576
LimitCORE=infinity
TimeoutStartSec=1min
Restart=on-abnormal

[Install]
WantedBy=multi-user.target
EOF
```
```
# 修改内核配置
# 低版本内核 kernel 3.10.x配置使用 overlay2
$ mkdir /etc/docker
$ cat > /etc/docker/daemon.json << "EOF"
{
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
EOF

$ mkdir -p /etc/systemd/system/docker.service.d/
$ cat > /etc/systemd/system/docker.service.d/docker-options.conf << "EOF"
[Service]
Environment="DOCKER_OPTS=--insecure-registry=10.254.0.0/16 --graph=/opt/docker --log-opt max-size=50m --log-opt max-file=5"
EOF

$ cat > /etc/systemd/system/docker.service.d/docker-dns.conf　<< "EOF"
[Service]
Environment="DOCKER_DNS_OPTIONS=\
    --dns 10.254.0.2 --dns 114.114.114.114  \
    --dns-search default.svc.cluster.local --dns-search svc.cluster.local  \
    --dns-opt ndots:2 --dns-opt timeout:2 --dns-opt attempts:2"
EOF
```
```
$ 启动docker
$ systemctl daemon-reload
$ systemctl start docker
$ systemctl enable docker
```

## 安装etcd集群
在211 212 213部署etcd集群
```
# 下载 二进制文件
$ cd /opt/package
$ wget https://github.com/coreos/etcd/releases/download/v3.2.11/etcd-v3.2.11-linux-amd64.tar.gz
$ tar zxvf etcd-v3.2.11-linux-amd64.tar.gz
$ cd etcd-v3.2.11-linux-amd64
$ cp etcd  etcdctl /usr/local/bin/
```

### 创建etcd证书
```
$ cd /opt/ssl

# etcd证书这里，默认配置了3个、如果后续需要增加节点、可以多配置几个以便后续添加能通过认证、不需要重新签发
$ cat > /opt/ssl/etcd-csr.json << "EOF"
{
  "CN": "etcd",
  "hosts": [
    "127.0.0.1",
    "192.168.153.211",
    "192.168.153.212",
    "192.168.153.213"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "ShenZhen",
      "L": "ShenZhen",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
EOF
```
```
# 生成etcd密钥
/opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \
  -ca-key=/opt/ssl/ca-key.pem \
  -config=/opt/ssl/config.json \
  -profile=kubernetes etcd-csr.json | /opt/local/cfssl/cfssljson -bare etcd

# 查看生成
$ ls etcd*
etcd.csr  etcd-csr.json  etcd-key.pem  etcd.pem

# 分发到etcd服务器
$ cp /opt/ssl/etcd*.pem /etc/kubernetes/ssl
$ for node in {192.168.153.211,192.168.153.212,192.168.153.213};do \
    rsync -avzp /opt/ssl/etcd*.pem $node:/etc/kubernetes/ssl/; \
done

# 如果非root用户、读取证书需要提权
$ chmod 644 /etc/kubernetes/ssl/etcd-key.pem
```
```

```
### 修改etcd配置
```
# 创建etcd data目录并授权
$ useradd etcd
$ mkdir -p /opt/etcd
$ chown -R etcd:etcd /opt/etcd

# 创建配置文件、每台节点需要修改--name 以及client ip
$ cat > /etc/systemd/system/etcd.service << "EOF"
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
WorkingDirectory=/opt/etcd/
User=etcd
# set GOMAXPROCS to number of processors
ExecStart=/usr/local/bin/etcd \
  --name=etcd1 \
  --cert-file=/etc/kubernetes/ssl/etcd.pem \
  --key-file=/etc/kubernetes/ssl/etcd-key.pem \
  --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \
  --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \
  --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
  --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
  --initial-advertise-peer-urls=https://192.168.153.211:2380 \
  --listen-peer-urls=https://192.168.153.211:2380 \
  --listen-client-urls=https://192.168.153.211:2379,http://127.0.0.1:2379 \
  --advertise-client-urls=https://192.168.153.211:2379 \
  --initial-cluster-token=k8s-etcd-cluster \
  --initial-cluster=etcd1=https://192.168.153.211:2380,etcd2=https://192.168.153.212:2380,etcd3=https://192.168.153.213:2380 \
  --initial-cluster-state=new \
  --data-dir=/opt/etcd/
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

# 默认2379端口是给client连接用的
# 而2380则在etcd集群各个节点之间交互用的
# 兼容老版本会同时开放4001 7001
```

### 启动并验证集群
```
$ systemctl daemon-reload
$ systemctl start etcd
$ systemctl enable etcd

# 在任意etcd节点验证集群
$ etcdctl \
        --cert-file=/etc/kubernetes/ssl/etcd.pem \
        --ca-file=/etc/kubernetes/ssl/ca.pem \
        --key-file=/etc/kubernetes/ssl/etcd-key.pem \
        cluster-health

# 查看etcd集群成员
$ etcdctl \
        --cert-file=/etc/kubernetes/ssl/etcd.pem \
        --ca-file=/etc/kubernetes/ssl/ca.pem \
        --key-file=/etc/kubernetes/ssl/etcd-key.pem \
        member list
```

> 集群的坑：新增节点会出现各种错误、要清理工作目录的member

## 配置kubernetes集群
### 下载安装组件
```
$ cd /opt/package
$ wget https://dl.k8s.io/v1.9.0/kubernetes-server-linux-amd64.tar.gz
$ tar -xzvf kubernetes-server-linux-amd64.tar.gz
$ cd kubernetes

# master-210
$ cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl} /usr/local/bin/

# node-211/212/213
$ cp -r server/bin/{kube-proxy,kubelet} /usr/local/bin/
```
### 配置master
Master需要部署kube-apiserver , kube-scheduler , kube-controller-manager这三个组件。 kube-scheduler作用是调度pods分配到那个node里，简单来说就是资源调度。 kube-controller-manager作用是对deployment controller , replication controller, endpoints controller, namespace controller, and serviceaccounts controller等等的循环控制，与kube-apiserver交互。
#### 创建admin证书
```
$ cd /opt/ssl
$ cat > /opt/ssl/admin-csr.json << "EOF"
{
  "CN": "admin",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "ShenZhen",
      "L": "ShenZhen",
      "O": "system:masters",
      "OU": "System"
    }
  ]
}
EOF

# 生成admin证书和秘钥
$ /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
  -ca-key=/etc/kubernetes/ssl/ca-key.pem \
  -config=/opt/ssl/config.json \
  -profile=kubernetes admin-csr.json | /opt/local/cfssl/cfssljson -bare admin

# 查看生成
$ ls admin*
admin.csr  admin-csr.json  admin-key.pem  admin.pem

$ cp admin*.pem /etc/kubernetes/ssl
```

#### 配置kubectl kubeconfig文件
生成证书相关的配置文件存储在/root/.kube 目录中
```
# 配置kubernetes集群
$ kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=https://192.168.153.210:6443

# 配置客户端认证
$ kubectl config set-credentials admin \
  --client-certificate=/etc/kubernetes/ssl/admin.pem \
  --embed-certs=true \
  --client-key=/etc/kubernetes/ssl/admin-key.pem

$ kubectl config set-context kubernetes \
  --cluster=kubernetes \
  --user=admin

$ kubectl config use-context kubernetes
```

#### 创建kubernetes证书
```
$ cd /opt/ssl
$ cat > /opt/ssl/kubernetes-csr.json << "EOF"
{
  "CN": "kubernetes",
  "hosts": [
    "127.0.0.1",
    "192.168.153.210",
    "10.254.0.1",
    "kubernetes",
    "kubernetes.default",
    "kubernetes.default.svc",
    "kubernetes.default.svc.cluster",
    "kubernetes.default.svc.cluster.local"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "ShenZhen",
      "L": "ShenZhen",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
EOF

## 这里hosts字段中三个IP分别为127.0.0.1本机， 192.168.153.211是Master的IP，多个Master需要写多个。10.254.0.1为 kubernetes SVC的IP， 一般是部署网络的第一个IP , 在启动完成后，我们使用kubectl get svc可以查看到
```
```
## 生成kubernetes证书和私钥
$ /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
  -ca-key=/etc/kubernetes/ssl/ca-key.pem \
  -config=/opt/ssl/config.json \
  -profile=kubernetes kubernetes-csr.json | /opt/local/cfssl/cfssljson -bare kubernetes

#查看生成
$ ls kubernetes*
kubernetes.csr  kubernetes-csr.json  kubernetes-key.pem  kubernetes.pem

# 拷贝到目录
$ cp kubernetes*.pem /etc/kubernetes/ssl/
```

#### 配置kube-apiserver验证kubelet
kubelet 首次启动时向kube-apiserver发送TLS Bootstrapping请求，kube-apiserver验证kubelet请求中的token是否与它配置的 token一致，如果一致则自动为kubelet生成证书和秘钥。
```
# 生成token
$ cd /etc/kubernetes/
$ export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')
$ echo "Token: ${BOOTSTRAP_TOKEN}"
Token: c2a318d6e2419fea976f1802926989ea

# 创建token.csv文件
$ cat > token.csv << EOF
${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,"system:kubelet-bootstrap"
EOF

# 生成高级审核配置文件
$ cat > audit-policy.yaml << "EOF"
# Log all requests at the Metadata level.
apiVersion: audit.k8s.io/v1beta1
kind: Policy
rules:
- level: Metadata
EOF
```

#### 创建kube-apiserver.service并启动
```
# 自定义系统service文件一般存于/etc/systemd/system/下
# 配置为各自的本地IP

$ cat > /etc/systemd/system/kube-apiserver.service << "EOF"
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
User=root
ExecStart=/usr/local/bin/kube-apiserver \
  --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \
  --advertise-address=192.168.153.210 \
  --allow-privileged=true \
  --apiserver-count=3 \
  --audit-policy-file=/etc/kubernetes/audit-policy.yaml \
  --audit-log-maxage=30 \
  --audit-log-maxbackup=3 \
  --audit-log-maxsize=100 \
  --audit-log-path=/var/log/kubernetes/audit.log \
  --authorization-mode=Node,RBAC \
  --bind-address=0.0.0.0 \
  --secure-port=6443 \
  --client-ca-file=/etc/kubernetes/ssl/ca.pem \
  --enable-swagger-ui=true \
  --etcd-cafile=/etc/kubernetes/ssl/ca.pem \
  --etcd-certfile=/etc/kubernetes/ssl/etcd.pem \
  --etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem \
  --etcd-servers=https://192.168.153.211:2379,https://192.168.153.212:2379,https://192.168.153.213:2379 \
  --event-ttl=1h \
  --kubelet-https=true \
  --insecure-bind-address=192.168.153.210 \
  --insecure-port=8080 \
  --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \
  --service-cluster-ip-range=10.254.0.0/17 \
  --service-node-port-range=30000-32767 \
  --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  --enable-bootstrap-token-auth \
  --token-auth-file=/etc/kubernetes/token.csv \
  --v=2
Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

# k8s1.8添加 --authorization-mode=Node
# k8s1.8添加 --admission-control=NodeRestriction
# k8s1.8添加 --audit-policy-file=/etc/kubernetes/audit-policy.yaml

# 这里面要注意的是--service-node-port-range=30000-32000
# 这个地方是映射外部端口时的端口范围，随机映射也在这个范围内映射，指定映射端口必须也在这个范围内。
```
```
# 启动kube-apiserver
$ systemctl daemon-reload
$ systemctl start kube-apiserver
$ systemctl enable kube-apiserver
```

#### 创建kube-controller-manager.service并启动
```
# 创建kube-controller-manager.service文件
$ cat > /etc/systemd/system/kube-controller-manager.service << "EOF"
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \
  --address=0.0.0.0 \
  --master=http://192.168.153.210:8080 \
  --allocate-node-cidrs=true \
  --service-cluster-ip-range=10.254.0.0/17 \
  --cluster-cidr=10.254.128.0/17 \
  --cluster-name=kubernetes \
  --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \
  --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \
  --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \
  --root-ca-file=/etc/kubernetes/ssl/ca.pem \
  --leader-elect=true \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```
```
# 启动kube-controller-manager
$ systemctl daemon-reload
$ systemctl start kube-controller-manager
$ systemctl enable kube-controller-manager
```

#### 创建kube-scheduler.service并启动
```
# 创建kube-cheduler.service文件
$ cat > /etc/systemd/system/kube-scheduler.service << "EOF"
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \
  --address=0.0.0.0 \
  --master=http://192.168.153.210:8080 \
  --leader-elect=true \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```
```
# 启动kube-scheduler
$ systemctl daemon-reload
$ systemctl start kube-scheduler
$ systemctl enable kube-scheduler
```

#### 验证master节点
```
$ kubectl get componentstatuses
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok                   
controller-manager   Healthy   ok                   
etcd-2               Healthy   {"health": "true"}   
etcd-1               Healthy   {"health": "true"}   
etcd-0               Healthy   {"health": "true"}
```

### 配置node
#### 配置kubelet
kubelet启动时向kube-apiserver发送TLS bootstrapping请求，需要先将bootstrap token文件中的kubelet-bootstrap用户赋予 system:node-bootstrapper角色，然后kubelet才有权限创建认证请求(certificatesigningrequests)。
```
# 先创建认证请求 此操作在master节点配置
# user master中token.csv文件里配置的用户
# 只需创建一次就可以

$ cd /opt/ssl
$ kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap
```

#### 创建kubelet kubeconfig文件
此操作在master节点配置
```
# 配置集群
$ kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=https://192.168.153.210:6443 \
  --kubeconfig=bootstrap.kubeconfig

# 配置客户端认证 token修改成自己的token值
$ kubectl config set-credentials kubelet-bootstrap \
  --token=33ca374f16c76b25367488152e07fbd5 \
  --kubeconfig=bootstrap.kubeconfig

# 配置关联
$ kubectl config set-context default \
  --cluster=kubernetes \
  --user=kubelet-bootstrap \
  --kubeconfig=bootstrap.kubeconfig

# 配置默认关联
$ kubectl config use-context default --kubeconfig=bootstrap.kubeconfig

# 分发生成的 bootstrap.kubeconfig 文件
$ cp bootstrap.kubeconfig /etc/kubernetes/

$ for node in {192.168.153.211,192.168.153.212,192.168.153.213};do \
    rsync -avzp /opt/ssl/bootstrap.kubeconfig $node:/etc/kubernetes/; \
done
```

#### 创建kubelet.service并启动
```
# 创建 kubelet 目录
# 配置node位各本机IP

$ mkdir /var/lib/kubelet
$ cat > /etc/systemd/system/kubelet.service << "HERE"
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/usr/local/bin/kubelet \
  --cgroup-driver=cgroupfs \
  --hostname-override=k8s-node-211 \
  --pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0 \ # jicki/pause-amd64:3.0
  --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \
  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \
  --cert-dir=/etc/kubernetes/ssl \
  --cluster_dns=10.254.0.2 \
  --cluster_domain=cluster.local. \
  --hairpin-mode promiscuous-bridge \
  --allow-privileged=true \
  --fail-swap-on=false \
  --serialize-image-pulls=false \
  --logtostderr=true \
  --max-pods=512 \
  --v=2

[Install]
WantedBy=multi-user.target
HERE

# 如上配置:
k8s-master-64    本机hostname
10.254.0.2       预分配的dns地址
cluster.local.   为kubernetes集群的domain
jicki/pause-amd64:3.0  这个是pod的基础镜像，即gcr的gcr.io/google_containers/pause-amd64:3.0 镜像，建议下载下来修改为自己的仓库中的比较快。
```
```
$ systemctl daemon-reload
$ systemctl start kubelet
$ systemctl enable kubelet
```

#### 配置TLS认证并验证nodes
```
# 查看csr的名称
$ kubectl get csr
NAME                                                   AGE       REQUESTOR           CONDITION
node-csr-0MB3vLXoRWwXyf4FW9axrpKrvYol0qGXorWWp-oIFW4   1m        kubelet-bootstrap   Pending

# 增加认证 如果多个csr 需要在命令结尾指定name
$ kubectl get csr | grep Pending | awk '{print $1}' | xargs kubectl certificate approve

# 验证nodes
$ kubectl get no
NAME           STATUS    ROLES     AGE       VERSION
k8s-node-211   Ready     <none>    5m        v1.9.1
k8s-node-212   Ready     <none>    2m        v1.9.1
k8s-node-213   Ready     <none>    2m        v1.9.1

# 成功以后会在个节点自动生成配置文件与密钥
$ ls /etc/kubernetes/kubelet.kubeconfig
/etc/kubernetes/kubelet.kubeconfig

$ ls /etc/kubernetes/ssl/kubelet*
/etc/kubernetes/ssl/kubelet-client.crt  /etc/kubernetes/ssl/kubelet.crt
/etc/kubernetes/ssl/kubelet-client.key  /etc/kubernetes/ssl/kubelet.key
```

### 配置kube-proxy
#### 创建kube-proxy证书
```
# 回到master端机器去配置证书，然后拷贝过来
$ cd /opt/ssl
$ cat > kube-proxy-csr.json << "HERE"
{
  "CN": "system:kube-proxy",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "ShenZhen",
      "L": "ShenZhen",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
HERE

# 生成kube-proxy证书和私钥
$ /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
  -ca-key=/etc/kubernetes/ssl/ca-key.pem \
  -config=/opt/ssl/config.json \
  -profile=kubernetes  kube-proxy-csr.json | /opt/local/cfssl/cfssljson -bare kube-proxy

# 查看证书
$ ls kube-proxy*
kube-proxy.csr  kube-proxy-csr.json  kube-proxy-key.pem  kube-proxy.pem

# 分发证书
$ cp /opt/ssl/kube-proxy*.pem /etc/kubernetes/ssl
$ for node in {192.168.153.211,192.168.153.212,192.168.153.213};do \
    rsync -avzp /opt/ssl/kube-proxy*.pem $node:/etc/kubernetes/ssl/; \
done
```

#### 创建kube-proxy kubeconfig文件
```
# 配置集群
$ kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=https://192.168.153.210:6443 \
  --kubeconfig=kube-proxy.kubeconfig

# 配置客户端认证
$ kubectl config set-credentials kube-proxy \
  --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \
  --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig

# 配置关联
$ kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig

# 配置默认关联
$ kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig

# 分发到需要的node端里
$ for node in {192.168.153.211,192.168.153.212,192.168.153.213};do \
    rsync -avzp /opt/ssl/kube-proxy.kubeconfig $node:/etc/kubernetes/; \
done
```

#### 创建kube-proxy.serviceg并启动
- kubernetes v1.9 官方ipvs已经beta, 开启ipvs测试一下.
- 官方 –feature-gates=SupportIPVSProxyMode=false默认是false,需要打开 –feature-gates=SupportIPVSProxyMode=true
–masquerade-all 必须添加这项配置，否则创建svc在ipvs会添加规则
- 打开ipvs需要安装ipvsadm软件，在node中安装`yum install ipvsadm -y`
```
# 创建 kube-proxy 目录
$ mkdir -p /var/lib/kube-proxy
```
```
$ cat > /etc/systemd/system/kube-proxy.service << "HERE"
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/usr/local/bin/kube-proxy \
  --bind-address=192.168.153.211 \
  --hostname-override=k8s-node-211 \
  --cluster-cidr=10.254.128.0/17 \
  --masquerade-all \
  --feature-gates=SupportIPVSProxyMode=true \
  --proxy-mode=ipvs \
  --ipvs-min-sync-period=5s \
  --ipvs-sync-period=5s \
  --ipvs-scheduler=rr \
  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \
  --logtostderr=true \
  --v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
HERE
```
```
# 启动 kube-proxy
$ systemctl daemon-reload
$ systemctl start kube-proxy
$ systemctl enable kube-proxy

# 检查ipvs
$ ipvsadm -L -n
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 rr persistent 10800
  -> 192.168.153.210:6443         Masq    1      0          0    
```
<br/>至此 Master 端 与 Master and Node 端的安装完毕<br/>

### 配置flannel网络
- 在多个node组成的kubernetes集群中，跨主机的网络互通是kubernetes正常工作的前提，kubernetes本身并不会对跨主机的容器网络进行设置，除了公有云平台提供的网络环境外，常见的开源工具有Flannel、OpenvSwitch、Weare、Calico等。kubernetes将优先使用CNI网络插件打通跨主机的容器网络。本实验采用flannel部署在kube-proxy相关的机器。

- Flannel是作为一个二进制文件的方式部署在每个node上，主要实现两个功能：
    - 为每个node分配subnet，容器将自动从该子网中获取IP地址
    - 当有node加入到网络中时，为每个node增加路由配置

#### 下载并安装
**安装配置flannel在mater和node节点 master因为后续要配置dashboard-heapster用到**
```
# 下载并安装
$ wget https://copr-be.cloud.fedoraproject.org/results/alebastr/k8s/fedora-27-x86_64/00677568-flannel/flannel-0.9.1-1.fc27.x86_64.rpm
$ rpm -ivh flannel-0.9.1-1.fc27.x86_64.rpm

# 由于之前更改了docker路径、所有重新拷贝
$ mv /usr/lib/systemd/system/docker.service.d/flannel.conf /etc/systemd/system/docker.service.d
```

#### 配置并启动
```
etcdctl --endpoints=https://192.168.153.211:2379,https://192.168.153.212:2379,https://192.168.153.213:2379 \
        --cert-file=/etc/kubernetes/ssl/etcd.pem \
        --ca-file=/etc/kubernetes/ssl/ca.pem \
        --key-file=/etc/kubernetes/ssl/etcd-key.pem \
        set /flannel/network/config \ '{"Network":"10.254.128.0/17","SubnetLen":24,"Backend":{"Type":"host-gw"}}'

# 修改flannel配置
$ vim /etc/sysconfig/flanneld    
# etcd url location.  Point this to the server where etcd runs
FLANNEL_ETCD_ENDPOINTS="https://192.168.153.211:2379,https://192.168.153.212:2379,https://192.168.153.213:2379"

# etcd config key.  This is the configuration key that flannel queries
# For address range assignment
#FLANNEL_ETCD_PREFIX="/atomic.io/network"
FLANNEL_ETCD_PREFIX="/flannel/network"

# Any additional options that you want to pass
#FLANNEL_OPTIONS=""
FLANNEL_OPTIONS="-ip-masq=true -etcd-cafile=/etc/kubernetes/ssl/ca.pem -etcd-certfile=/etc/kubernetes/ssl/etcd.pem -etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem -iface=ens33"

# 启动flannel
$ systemctl daemon-reload
$ systemctl start flanneld
$ systemctl enable flanneld

# 重启docker kubelet
# 验证网络 ifconfig 查看docker0是否已经更改为配置的IP网段
```

#### 测试集群
```
$ mkdir /opt/test
$ cd /opt/test
$ cat > ./nginx_test.yaml << "HERE"
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-dm
spec:
  replicas: 2
  template:
    metadata:
      labels:
        name: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:alpine
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 80

---

apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
  selector:
    name: nginx
HERE

$ kubectl apply -f nginx_test.yaml
$ kubectl get po -o wide
NAME                        READY     STATUS    RESTARTS   AGE       IP             NODE
nginx-dm-84f8f49555-p5kzz   1/1       Running   0          4m        10.254.112.2   k8s-node-211
nginx-dm-84f8f49555-snwlk   1/1       Running   0          4m        10.254.74.2    k8s-node-213

$ kubectl get svc -o wide
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE       SELECTOR
kubernetes   ClusterIP   10.254.0.1      <none>        443/TCP   20h       <none>
nginx-svc    ClusterIP   10.254.59.121   <none>        80/TCP    9m        name=nginx

# 在安装flannel网络的节点里测试访问
$ curl 10.254.59.121
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>

# 查看ipvs规则
$ ipvsadm -L -n
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 rr persistent 10800
  -> 192.168.153.210:6443         Masq    1      0          0         
TCP  10.254.59.121:80 rr
  -> 10.254.74.2:80               Masq    1      0          1         
  -> 10.254.112.2:80              Masq    1      0          0     
```

### 配置CoreDNS
官方地址：https://coredns.io

#### 下载镜像
```
# 官方镜像 本文所用就是这个镜像
coredns/coredns:latest

# 作者的镜像
jicki/coredns:latest
```

#### 创建yaml文件
```
mkdir /opt/coredns
cat > /opt/coredns/coredns.yaml << "HERE"
apiVersion: v1
kind: ServiceAccount
metadata:
  name: coredns
  namespace: kube-system
  labels:
      kubernetes.io/cluster-service: "true"
      addonmanager.kubernetes.io/mode: Reconcile
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
    addonmanager.kubernetes.io/mode: Reconcile
  name: system:coredns
rules:
- apiGroups:
  - ""
  resources:
  - endpoints
  - services
  - pods
  - namespaces
  verbs:
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
    addonmanager.kubernetes.io/mode: EnsureExists
  name: system:coredns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:coredns
subjects:
- kind: ServiceAccount
  name: coredns
  namespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        log
        health
        kubernetes cluster.local 10.254.0.0/17
        proxy . /etc/resolv.conf
        cache 30
    }
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: coredns
    kubernetes.io/cluster-service: "true"
    kubernetes.io/name: "CoreDNS"
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: coredns
  template:
    metadata:
      labels:
        k8s-app: coredns
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
        scheduler.alpha.kubernetes.io/tolerations: '[{"key":"CriticalAddonsOnly", "operator":"Exists"}]'
    spec:
      serviceAccountName: coredns
      containers:
      - name: coredns
        image: coredns/coredns:latest   # jicki/coredns:latest
        imagePullPolicy: Always
        args: [ "-conf", "/etc/coredns/Corefile" ]
        volumeMounts:
        - name: config-volume
          mountPath: /etc/coredns
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
      dnsPolicy: Default
      volumes:
        - name: config-volume
          configMap:
            name: coredns
            items:
            - key: Corefile
              path: Corefile
---
apiVersion: v1
kind: Service
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: coredns
    kubernetes.io/cluster-service: "true"
    kubernetes.io/name: "CoreDNS"
spec:
  selector:
    k8s-app: coredns
  clusterIP: 10.254.0.2
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP
HERE
```

#### 启动并验证
```
# 导入yaml文件
$ kubectl apply -f coredns.yaml
serviceaccount "coredns" created
clusterrole "system:coredns" created
clusterrolebinding "system:coredns" created
configmap "coredns" created
deployment "coredns" created
service "coredns" created

# 查看coredns服务
$ kubectl get po,svc -n kube-system
NAME                          READY     STATUS    RESTARTS   AGE
po/coredns-6bd7d5dbb5-x2zhp   1/1       Running   0          2m

NAME          TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE
svc/coredns   ClusterIP   10.254.0.2   <none>        53/UDP,53/TCP   4h

# 检查日志
$ kubectl logs -n kube-system coredns-6bd7d5dbb5-x2zhp
.:53
CoreDNS-1.0.6
linux/amd64, go1.10, 83b5eadb
2018/02/27 06:59:20 [INFO] CoreDNS-1.0.6
2018/02/27 06:59:20 [INFO] linux/amd64, go1.10, 83b5eadb

# 验证 dns 服务
# 在验证dns之前，在dns未部署之前创建的 pod 与 deployment 等，都必须删除重新部署，否则无法解析

$ cat > /opt/test/test_dns.yaml << "HERE"
apiVersion: v1
kind: Pod
metadata:
  name: alpine
spec:
  containers:
  - name: alpine
    image: alpine
    command:
    - sh
    - -c
    - while true; do sleep 1; done
HERE

# 查看创建的服务
[root@k8s-master-210 test]# kubectl get po,svc
NAME                           READY     STATUS    RESTARTS   AGE
po/alpine                      1/1       Running   0          4h
po/nginx-dm-84f8f49555-4vb2c   1/1       Running   0          3h
po/nginx-dm-84f8f49555-q8x5s   1/1       Running   0          3h

NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
svc/kubernetes   ClusterIP   10.254.0.1      <none>        443/TCP   1d
svc/nginx-svc    ClusterIP   10.254.59.121   <none>        80/TCP    5h

# 测试
$ kubectl exec -it alpine nslookup nginx-svc
nslookup: can't resolve '(null)': Name does not resolve
Name:      nginx-svc
Address 1: 10.254.59.121 nginx-svc.default.svc.cluster.local

$ kubectl exec -it alpine nslookup kubernetes
nslookup: can't resolve '(null)': Name does not resolve
Name:      kubernetes
Address 1: 10.254.0.1 kubernetes.default.svc.cluster.local
```

### 配置Ingress
#### 部署 nginx ingress
- Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress
- 什么是 Ingress ? Ingress 就是利用 Nginx Haproxy 等负载均衡工具来暴露 Kubernetes 服务。
- 官方 Nginx Ingress github: https://github.com/kubernetes/ingress-nginx/

#### 配置调度node
- ingress 有多种方式
    - deployment自由调度replicas
    - daemonset全局调度分配到所有node里

##### 对node进行标签
```
#  deployment自由调度过程中，由于我们需要约束controller调度到指定的node中，所以需要对node进行label标签
# 默认如下:
$ kubectl get nodes
NAME           STATUS    ROLES     AGE       VERSION
k8s-node-211   Ready     <none>    2d        v1.9.1
k8s-node-212   Ready     <none>    2d        v1.9.1
k8s-node-213   Ready     <none>    2d        v1.9.1

# 对211 212 213打上label
$ kubectl label nodes k8s-node-211 ingress=proxy
$ kubectl label nodes k8s-node-212 ingress=proxy
$ kubectl label nodes k8s-node-213 ingress=proxy
node "k8s-node-213" labeled


# 打完标签以后
$ kubectl get nodes --show-labels
NAME           STATUS    ROLES     AGE       VERSION   LABELS
k8s-node-211   Ready     <none>    2d        v1.9.1    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=k8s-node-211
k8s-node-212   Ready     <none>    2d        v1.9.1    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=k8s-node-212
k8s-node-213   Ready     <none>    2d        v1.9.1    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=k8s-node-213
```

##### 部署ingress
- 现在traefik已经取代了ingress，所以直接部署traefik即可。
- ingress 就是为了解决Pod漂移、端口管理、域名分配以动态更新问题
- ingress原理
    - 在讲Ingres实现原理时，我们可以从NodePort入手，NodePort最大的弊端就是端口多了之后难以管理，我们自然而然的就会想到利用反向代理工具（比如 nginx）只监听host上一个端口，然后再根据请求的域名转发给集群内部服务，这就要求这个nginx能够转发到集群内部，这个简单，我们直接将nginx部署到集群内部就可以了。接下来的问题就是如何配置nginx了，这就要借助k8s中的ingress了，ingress实际上就是一个yaml文件，真正执行配置nginx的是叫做ingress controller的程序，它会调用k8s 的api，获取ingress，然后根据这个yaml文件生成nginx 配置模板，写入nginx。除此之外，Ingress Controller 通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如新增和减少 pod，service 增加与减少等；当得到这些变化信息后，Ingress Controller 再结合 Ingress 生成配置，然后更新反向代理负载均衡器，并刷新其配置，达到服务发现的目的。
      ```

                   ingress controller ---> API 交互 <---> kubernetes API
                         |            ---> 读取域名解析规则 <---> ingress(定义了域|名匹配service的规则)
                         |                
                   动态写规则(配置)
                         |
                         v
      外部请求 ---> 负载均衡器      

      ```

    - ingress包括Ingress Controller，ingress两部分，但是通常反向代理负载均衡器与Ingress Controller会部署到同一个pod中，一个负责反向代理，一个负责与k8s交互并更新配置。不过随着微服务的流行，有人将这两个功能合在了一块，就是traefik。

      ```

      外部请求 ---> traefik ---> API 交互 <---> kubernetes API
                              ---> 读取域名解析规则 <---> ingress(定义了域名匹配service的规则)

      ```

- 下载镜像
    - 官方镜像
        - gcr.io/google_containers/defaultbackend:1.4
        - quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.9.0

    - 国内镜像
        - jicki/defaultbackend:1.4
        - jicki/nginx-ingress-controller:0.9.0

```
# 部署Nginx backend用于统一转发没有的域名到指定页面
$ curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/namespace.yaml
$ curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/default-backend.yaml
$ curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/configmap.yaml
$ curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/tcp-services-configmap.yaml
$ curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/udp-services-configmap.yaml

# 部署Ingress RBAC认证
$ curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/rbac.yaml

# 部署Ingress Controller组件
$ curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/with-rbac.yaml

# 替换所有的images
$ sed -i 's/gcr\.io\/google_containers/jicki/g' *
$ sed -i 's/quay\.io\/kubernetes-ingress-controller/jicki/g' *

# 上面对三个 node打了label所以配置 replicas: 3
# 修改yaml文件增加rbac认证 , hostNetwork还有 nodeSelector, 第二个spec下增加

$ vim with-rbac.yaml
spec:
  replicas: 2
  ....
    spec:
      serviceAccountName: nginx-ingress-serviceaccount
      hostNetwork: true
      nodeSelector:
        ingress: proxy
    ....


# 导入yaml文件
$ kubectl apply -f namespace.yaml
namespace "ingress-nginx" created

$ kubectl apply -f .
configmap "nginx-configuration" created
deployment "default-http-backend" created
service "default-http-backend" created
namespace "ingress-nginx" configured
serviceaccount "nginx-ingress-serviceaccount" created
clusterrole "nginx-ingress-clusterrole" created
role "nginx-ingress-role" created
rolebinding "nginx-ingress-role-nisa-binding" created
clusterrolebinding "nginx-ingress-clusterrole-nisa-binding" created
configmap "tcp-services" created
configmap "udp-services" created
deployment "nginx-ingress-controller" created


# 查看服务，可以看到这三个pods被分别调度到211 212 213
$ kubectl get pods -n ingress-nginx -o wide
NAME                                        READY     STATUS    RESTARTS   AGE       IP                NODE
default-http-backend-55c6c69b88-xxgfp       1/1       Running   0          19m       10.254.79.2       k8s-node-212
nginx-ingress-controller-59c6f88d59-9n8mk   1/1       Running   0          19m       192.168.153.213   k8s-node-213
nginx-ingress-controller-59c6f88d59-p7rzr   1/1       Running   0          19m       192.168.153.211   k8s-node-211
nginx-ingress-controller-59c6f88d59-sffhg   1/1       Running   2          1d        192.168.153.212   k8s-node-212


# 创建一个 基于 nginx-dm 的 ingress
$ cat > /opt/test/nginx-ingress.yaml <<"HERE"
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nginx-ingress
spec:
  rules:
  - host: nginx.jicki.me
    http:
      paths:
      - backend:
          serviceName: nginx-svc
          servicePort: 80
HERE

$ kubectl apply -f nginx-ingress.yaml

# 查看服务
$ kubectl get ingress
NAME            HOSTS      ADDRESS   PORTS     AGE
nginx-ingress   test.com             80        1d

# 绑定任意master/node ip到/etc/hosts 测试访问
$ curl nginx.jicki.me
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

### 配置traefik
traefik(https://traefik.io/) 是一款开源的反向代理与负载均衡工具。它最大的优点是能够与常见的微服务系统直接整合，可以实现自动化动态配置。目前支持 Docker, Swarm, Mesos/Marathon, Mesos, Kubernetes, Consul, Etcd, Zookeeper, BoltDB, Rest API 等等后端模型。在Kubernetes集群中使用，可以完全替代 ngxin + Ingress Controller，快速实现服务的暴漏。

#### 部署测试用的nginx
```
$ cat > /opt/test/nginx1-7.yaml <<"HERE"
apiVersion: v1
kind: Service
metadata:
  name: frontend
spec:
  ports:
    - port: 80
      targetPort: 80
  selector:
    app: nginx1-7
---
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx1-7-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx1-7
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
HERE

$ cat > /opt/test/nginx1-8.yaml <<"HERE"
apiVersion: v1
kind: Service
metadata:
  name: my-nginx
spec:
  ports:
    - port: 80
      targetPort: 80
  selector:
    app: nginx1-8
---
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx1-8-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx1-8
    spec:
      containers:
      - name: nginx
        image: nginx:1.8
        ports:
        - containerPort: 80
HERE
```

#### 开启RBAC授权需要使用角色和绑定角色
```
$ cat > ingress-rbac.yaml <<"HERE"
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ingress
  namespace: kube-system

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: ingress
subjects:
  - kind: ServiceAccount
    name: ingress
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
HERE
```

#### 部署traefik-deploy
traefik监听node的80和8580端口，80提供正常服务，8580是其自带的UI界面，原本默认是8080，因为环境里端口冲突这里临时改一下。
```
$ cat > traefik-deploy.yaml <<"HERE"
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: traefik-ingress-lb
  namespace: kube-system
  labels:
    k8s-app: traefik-ingress-lb
spec:
  template:
    metadata:
      labels:
        k8s-app: traefik-ingress-lb
        name: traefik-ingress-lb
    spec:
      terminationGracePeriodSeconds: 60
      hostNetwork: true
      restartPolicy: Always
      serviceAccountName: ingress
      containers:
      - image: traefik
        name: traefik-ingress-lb
        resources:
          limits:
            cpu: 200m
            memory: 30Mi
          requests:
            cpu: 100m
            memory: 20Mi
        ports:
        - name: http
          containerPort: 80
          hostPort: 80
        - name: admin
          containerPort: 8580
          hostPort: 8580
        args:
        - --web
        - --web.address=:8580
        - --kubernetes  
HERE
```

#### 部署traefik-ingress
```
$ cat > traefik-ingress.yaml <<"HERE"
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: traefik-ingress
  namespace: default
spec:
  rules:
  - host: traefik.nginx.io
    http:
      paths:
      - path: /
        backend:
          serviceName: my-nginx
          servicePort: 80
  - host: traefik.frontend.io
    http:
      paths:
      - path: /
        backend:
          serviceName: frontend
          servicePort: 80
HERE

# backend中要配置default namespace中启动的service名字，如果你没有配置namespace名字，默认使用default namespace，如果你在其他namespace中创建服务想要暴露到kubernetes集群外部，可以创建新的ingress.yaml文件，同时在文件中指定该namespace，其他配置与上面的文件格式相同。path就是URL地址后的路径，如traefik.frontend.io/path，service将会接受path这个路径，host最好使用service-name.filed1.filed2.domain-name这种类似主机名称的命名方式，方便区分服务。有新service增加时，修改该文件后可以使用kubectl replace -f traefik.yaml来更新。
```

#### 部署Traefik-UI
```
$ cat > traefik-ui-service.yaml <<"HERE"
apiVersion: v1
kind: Service
metadata:
  name: traefik-web-ui
  namespace: kube-system
spec:
  selector:
    k8s-app: traefik-ingress-lb
  ports:
  - name: web
    port: 80
    targetPort: 8580
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: traefik-web-ui
  namespace: kube-system
spec:
  rules:
  - host: traefik-ui.local
    http:
      paths:
      - path: /
        backend:
          serviceName: traefik-web-ui
          servicePort: web
HERE
```

#### 启动并测试
```
$ kubectl apply -f .
$ kubectl get pods -n kube-system -l k8s-app=traefik-ingress-lb -o wide  
NAME                                 READY     STATUS    RESTARTS   AGE       IP                NODE
traefik-ingress-lb-cc54bf594-qmv8w   1/1       Running   0          3h        192.168.153.213   k8s-node-213

# 浏览器输入http://192.168.153.213:8580/dashboard/#/ 就可以看到界面，左侧黄色是rule，右侧绿色是backend

$ kubectl get pods -n kube-system -o wide  
NAME                                 READY     STATUS    RESTARTS   AGE       IP          
coredns-6bd7d5dbb5-lgspq             1/1       Running   0          5d        10.254.132.3
traefik-ingress-lb-cc54bf594-qmv8w   1/1       Running   0          3h        192.168.153

绑定hosts 访问 traefik.frontend.io
```

#### 监控检查
关于健康检查，测试可以使用kubernetes的Liveness Probe实现，如果LivenessProbe检查失败，则traefik会自动移除该pod


### 配置Dashboard和Heapster
#### 配置dashboard
##### 下载镜像
```
# 官方镜像
https://github.com/kubernetes/dashboard/releases

# 本次采用官方1.8.0
gcr.io/google_containers/kubernetes-dashboard-amd64:v1.8.0

# 国内镜像
jicki/kubernetes-dashboard-amd64:v1.8.1
```

##### 下载并导入yaml文件
- 创建kubernetes-dashboard.yaml
    - https://github.com/MyNote/tree/master/Container/K8s/configuration-files/dashboard-heapster/dashboard/kubernetes-dashboard.yaml
    - spec.containers.image：使用官方镜像1.8.0
    - spec.containers.args：本次kubernetes1.9.1是通过HTTPS安全验证的安装，访问https://masterip:6443，所以此处添加--auto-generate-certificates用来自动生成dashboard证书

---
- 创建kubernetes-dashboard-rbac.yaml
    - 因为kubernetes1.9.1开启了RBAC，所以这里需要创建一个RBAC认证
    - https://github.com/MyNote/tree/master/Container/K8s/configuration-files/dashboard-heapster/dashboard/kubernetes-dashboard-rbac.yaml

##### dashboard配置启动
```
# 创建一个空目录certs，创建认证
$ kubectl create secret generic kubernetes-dashboard-certs --from-file=certs -n kube-system

# 将kubernetes-dashboard.yaml、kubernetes-rbac.yaml放置到同一个目录并启动
$ ls
certs  kubernetes-dashboard-rbac.yaml  kubernetes-dashboard.yaml

$ kubectl apply -f .

# 查看启动后的pod以及详情
$ kubectl get pods --namespace="kube-system"
NAME                                    READY     STATUS    RESTARTS   AGE
kubernetes-dashboard-57889f9586-6d8mx   1/1       Running   7          18h

$ kubectl describe -n kube-system pod kubernetes-dashboard-57889f9586-6d8mx

# 访问以下链接（1.8.0无法访问 https://masterip:6443/ui)
$ kubectl cluster-info
# https://MasterIP:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/

# 登录界面(token方式)
$ kubectl get secret -n kube-system
$ kubectl describe secret kubernetes-dashboard-token-j9dpf -n kube-system
# 一定要是kubernetes-dashboard-token这个secret的token才有权限、其他的显示forbidden。
```

##### 解决各种报错
首次安装，如果没有做apiserver参数配置，则可能会出现一些问题。下面就看下常见问题的解决方法
1. 403报错
  ```
  {
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {

  },
  "status": "Failure",
  "message": "services /"https:kubernetes-dashboard:/" is forbidden: User /"system:anonymous/" cannot get services/proxy in the namespace /"kube-system/"",
  "reason": "Forbidden",
  "details": {
    "name": "https:kubernetes-dashboard:",
    "kind": "services"
  },
  "code": 403
}
  ```
    Kubernetes API Server新增了–anonymous-auth选项，允许匿名请求访问secure port。没有被其他authentication方法拒绝的请求即Anonymous requests， 这样的匿名请求的username为system:anonymous, 归属的组为system:unauthenticated。并且该选线是默认的。这样一来，当采用chrome浏览器访问dashboard UI时很可能无法弹出用户名、密码输入对话框，导致后续authorization失败。为了保证用户名、密码输入对话框的弹出，需要将–anonymous-auth设置为false。

  ```
  # 解决方法：在api-server配置文件中添加--anonymous-auth=false
  $ vim /etc/systemd/system/kube-apiserver.service
  19   --anonymous-auth=false \ # 不接受匿名访问，若为true，则表示接受，此处设置为false，便于dashboard访问

  $ systemctl daemon-reload
  $ systemctl restart kube-apiserver
  ```

2. 401报错
  ```
  {
    "kind": "Status",
    "apiVersion": "v1",
    "metadata": {

    },
    "status": "Failure",
    "message": "Unauthorized",
    "reason": "Unauthorized",
    "code": 401
  }
  ```

  ```
  # 解决方法：
  # 创建base认证、并添加到启动文件(格式:pwd:user:uid)
  $ cat > /etc/kubernetes/basic-auth.file <<"EOF"
  admin,admin,1002
  EOF

  $ vim /etc/systemd/system/kube-apiserver.service
  20   --basic-auth-file=/etc/kubernetes/basic-auth.file \

  $ systemctl daemon-reload && systemctl restart kube-apiserver

  # 最后将访问账号名admin与kubernetes-dashboard-rbac.yaml文件中指定的cluster-admin关联，获得访问权限
  $ kubectl create clusterrolebinding login-dashboard-admin --clusterrole=cluster-admin --user=admin
  ```

3. 503报错
- 如果安装的docker版本为1.13及以上，并且网络畅通，flannel、etcd都正常，但还是会出现`getsockopt: connection timed out`的错误，则可能是iptables配置问题。

- docker从1.13版本开始，可能将iptables FORWARD chain的默认策略设置为DROP，从而导致ping其他Node上的Pod IP失败，遇到这种问题时，需要手动设置策略为ACCEPT：`sudo iptables -P FORWARD ACCEPT`，并且要在docker启动前执行，所以直接修改docker启动文件

  ```
   $ vim /etc/systemd/system/docker.service
   [Service]
   Type=notify
   Environment=GOTRACEBACK=crash
   ExecStartPost=/sbin/iptables -I FORWARD -s 0.0.0.0/0 -j ACCEPT  # 增加iptables
   ExecReload=/bin/kill -s HUP $MAINPID
   Delegate=yes
   KillMode=process
   ExecStart=/usr/bin/dockerd \

   $ systemctl daemon-reload && systemctl restart docker

   # 如果还有错误、检查防火墙的filter表规则
  ```


#### 配置heapster
##### 下载源码包
```
# 官方地址：本次采用的是v1.2.0
https://github.com/kubernetes/heapster/releases

# 本次所用到的镜像
kubernetes/heapster:canary
kubernetes/heapster_influxdb:v0.5
signalive/heapster_grafana:2.6.0-2
```

##### 配置并启动
```
# 以InfluxDB为存储后端的Heapster部署yaml在deploy/kube-config/influxdb，我们拷贝到自己的目录,heapster源码库中最新的代码已经有所不同，最新代码将influxdb和grafana从influxdb-grafana-controller.yaml拆分开来
$ cp -r heapster-1.2.0/deploy/kube-config/influxdb/* /opt/dashboard-heapster/heapster/

# 修改heapster-controller.yaml配置文件
containers:
      - name: heapster
        image: 10.0.11.222:5000/bigdata/kubernetes/heapster:canary
        volumeMounts:
        - mountPath: /root/.kube
          name: config
        imagePullPolicy: Always
        command:
        - /heapster
        - --source=kubernetes:https://kubernetes.default?inClusterConfig=false&insecure=true&auth=/root/.kube/config
        - --sink=influxdb:http://monitoring-influxdb:8086
      volumes:
      - name: config
        hostPath:
          path: /root/.kube

1. 将本地/root/.kube目录与容器中该目录挂载，使用/root/.kube目录下的config文件
2、–source=kubernetes:https://kubernetes.default?inClusterConfig=false&insecure=true&auth=/root/.kube/config：修改原来–source为现在这个链接，用于连接apiserver获取度量信息。

# 将master的kube目录分发到node
$ for node in {192.168.153.211,192.168.153.212,192.168.153.213};do \
    scp -r /root/.kube/ root@192.168.153.213:/root/
done

# 如果pod状态成功、就可以在dashboard看到效果。
```

#### 配置efk
##### 待补充

## k8s运维相关
### 基础维护
```
# 当需要对主机进行维护升级时，首先将节点主机设置成不可调度模式：
$ kubectl cordon［nodeid］  

# 然后需要将主机上正在运行的容器驱赶到其它可用节点：
$ kubectl drain ［nodeid］

# 给予900秒宽限期优雅的调度
$ kubectl drain node1.k8s.novalocal --grace-period=120

# 当容器迁移完毕后，运维人员可以对该主机进行操作，配置升级性能参数调优等等。当对主机的维护操作完毕后， 再将主机设置成可调度模式：
$ kubectl uncordon [nodeid]  
```

## Other
### 特殊env
```
# yaml 中的一些 特殊 env
    env:
    - name: MY_POD_NAME
      valueFrom:    
        fieldRef:
          fieldPath: metadata.name
    - name: MY_POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    - name: MY_POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
```


> 参考文档：https://jicki.me/2017/12/20/kubernetes-1.9-ipvs/#发布证书

> 参考文档：https://www.kubernetes.org.cn/3336.html

> 参考文档：https://www.kubernetes.org.cn/3336.html

> ingress:https://mritd.me/2017/03/04/how-to-use-nginx-ingress/

> dashboard: https://o-my-chenjian.com/2017/04/08/Deploy-Dashboard-With-K8s/

> kubeadm国内：http://blog.csdn.net/aixiaoyang168/article/details/78411511

> traefik: https://jkzhao.github.io/2017/09/25/%E5%9C%A8Kubernetes%E4%B8%8A%E4%BD%BF%E7%94%A8Traefik/
